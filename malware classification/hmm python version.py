# %%
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from IPython.display import clear_output

# %%
malwares = pd.read_csv("dynamic_api_call_sequence_per_malware_100_0_306.csv")
malwares

# %%
malwares = malwares.dropna(subset='malware')

# %%
test = malwares[malwares['malware'] == 1].iloc[:,1:101].to_numpy()
randomised_test = test.copy()


from sklearn.cluster import KMeans
y=[]
z=[]
max_clust = 31
for i in range(2,max_clust):
    model = KMeans(n_clusters=i, random_state=0, n_init='auto')
    model.fit(test)
    y.append(model.inertia_)
for i in range(2,max_clust):
    model = KMeans(n_clusters=i, random_state=0, n_init='auto')
    model.fit(randomised_test)
    z.append(model.inertia_)
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
plt.plot(range(2, max_clust), y, label='test')
plt.plot(range(2, max_clust), z, label='randomised_test')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method for Choosing Number of Clusters')
plt.legend()
plt.show() 
"""
plt.plot(range(2,max_clust),y)
plt.plot(range(2,max_clust),z)"""

# %%
test = malwares[malwares['malware'] == 1].iloc[:,1:101].to_numpy()
print('unique test:',np.unique(test))
print(np.unique(test).size)

def transle_to_norm(s):
    u = np.unique(s)
    l_u = u.size
    translator = dict()
    for w,i in zip(u, range(l_u)):
        translator[w]=i
    translaed_s = [[translator[w] for w in row]for row in s]

    return np.array(translaed_s), l_u

x,l=transle_to_norm(test)
print('x:',np.unique(x))
print('l:',l)


randomised_test = test.copy()
print(type(randomised_test))
print(test.shape)
np.random.shuffle(randomised_test)
print('test shape:', test.shape)
#np.reshape(randomised_test,test.shape)
print('randomised test shape:', randomised_test.shape)
print('random:',randomised_test[:2])
print('test:', test[:2])

# %%
def Baum_Welch_Norm_step(a,b,init,obs) :
    N=a.shape[0]
    M=b.shape[0]
    T=len(obs)
    cs=[]

    # compute the alphas
    alphas=np.zeros([a.shape[0],len(obs)])
    alpha=(b[obs[0],:].ravel())*(init.ravel())
    #print("alpha : ",alpha)
    c=np.sum(alpha)
    #print("c = ",c)
    cs.append(1/c)
    alpha/=c
    alphas[:,0]=alpha
    #print("alphas : ",alphas)
    for i in range(1,len(obs)) :
        alpha= b[obs[i],:]*np.matmul(a,alpha)
        #print(i,"alpha =", alpha)
        c=np.sum(alpha)
        #print(i,"c =", c)
        cs.append(1/c)
        alpha/=c
        alphas[:,i]=alpha


    # Compute the betas

    betas=np.zeros([a.shape[0],T])
    beta=np.ones(a.shape[0])*cs[T-1]
    betas[:,T-1]=beta
    for i in range(1,T-1) :
        beta=np.matmul(b[obs[T-i],:]*beta,a)
        beta*=cs[T-i-1]
        betas[:,T-i-1]=beta


    #Compute the lambdas
    lambdas=np.zeros([N,N,T-1])
    for s in range(T-1) :
        lambdas[:,:,s]=np.transpose((betas[:,s+1]*b[obs[s+1],:])*np.transpose(a*alphas[:,s]))


    #Compute the gammas
    gammas=np.sum(lambdas, axis=0)

    #Compute new B
    sumgammas=np.sum(gammas,axis=1)
    new_B=np.zeros([M,N])
    for i in range(M) :
        mask=np.array([o==i for o in obs[:-1] ])*1.0
        new_B[i,:]=(np.sum(gammas*mask, axis=1)/sumgammas)

    #Compute new A
    sumlambd=np.sum(lambdas,axis=2)
    new_A=sumlambd/sumgammas

    #Compute new init
    new_init=gammas[:,0]

    #Compute log likehood
    log_lik=-1*np.sum(np.log(np.array(cs)))

    return new_A,new_B,new_init,log_lik


# %%
def random_init_prob_cols(n_rows,n_cols) :
    answer=np.random.randn(n_cols*n_rows).reshape((n_rows,n_cols))
    answer=np.exp(answer)
    answer/=np.sum(answer,axis=0)[None,:]
    return answer

def train_HMM_on_seq_of_api_calls(line_as_number,n_hidden, n_obs):

    
    est_A=random_init_prob_cols(n_hidden,n_hidden)
    est_B=random_init_prob_cols(n_obs,n_hidden)
    est_init=np.ones(n_hidden)/n_hidden

    for _ in range(15) :
        est_A,est_B,est_init,log_lik=Baum_Welch_Norm_step(est_A,est_B,est_init,line_as_number)
        est_B = np.maximum(est_B,.000001)
        est_B /= (est_B.sum(axis=0)[None,:])
        print("  ----->",_, " : " ,log_lik)
    
    return est_A, est_B, est_init



# %%
def log_lik(a,b,init,obs):
    N=a.shape[0]
    M=b.shape[0]
    T=len(obs)
    cs=[]

    # compute the alphas
    alphas=np.zeros([a.shape[0],len(obs)])
    alpha=(b[obs[0],:].ravel())*(init.ravel())
    #print("alpha : ",alpha)
    c=np.sum(alpha)
    #print("c = ",c)
    cs.append(1/c)
    alpha/=c
    alphas[:,0]=alpha
    #print("alphas : ",alphas)
    for i in range(1,len(obs)) :
        alpha= b[obs[i],:]*np.matmul(a,alpha)
        #print(i,"alpha =", alpha)
        c=np.sum(alpha)
        #print(i,"c =", c)
        cs.append(1/c)
        alpha/=c
        alphas[:,i]=alpha

    log_lik=-1*np.sum(np.log(np.array(cs)))
    return log_lik

# %%
def get_hmm_f_for_row(row, n_hidden, n_obs):
    a, b , init = train_HMM_on_seq_of_api_calls(row, n_hidden, n_obs)
    init=np.ones(n_hidden)/n_hidden
    def log_lik_hmm(other_row):
        return log_lik(a,b,init,other_row)
    return log_lik_hmm

f =get_hmm_f_for_row(x[10],2,l )
g =get_hmm_f_for_row(x[50],2,l )
h =get_hmm_f_for_row(x[99],2,l )

""" for i in range(1,l) : 
    f = get_hmm_f_for_row(x[i],2,l)
 """

# %%
#%matplotlib
L= [f(x[i]) for i in range(l)]
M= [g(x[i]) for i in range(l)]
N = [h(x[i]) for i in range(l)]
fig = plt.figure()
ax = fig.add_subplot(projection= '3d')

ax.scatter(L,M,N)
plt.show()


